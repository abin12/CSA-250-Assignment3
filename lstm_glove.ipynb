{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled6.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "KB4QLCk4dM00",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from __future__ import print_function\n",
        "from functools import reduce\n",
        "import json\n",
        "import os\n",
        "import re\n",
        "import tarfile\n",
        "import tempfile\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1337)  # for reproducibility\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dxWoLF9phmAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import keras\n",
        "import keras.backend as K\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from keras.layers import merge, recurrent, Dense, Input, Dropout, TimeDistributed\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.wrappers import Bidirectional\n",
        "from keras.models import Model\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.regularizers import l2\n",
        "from keras.utils import np_utils"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZf3Mw2fhpcO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "def extract_tokens_from_binary_parse(parse):\n",
        "    return parse.replace('(', ' ').replace(')', ' ').replace('-LRB-', '(').replace('-RRB-', ')').split()\n",
        "\n",
        "def yield_examples(fn, skip_no_majority=True, limit=None):\n",
        "  for i, line in enumerate(open(fn)):\n",
        "    if limit and i > limit:\n",
        "      break\n",
        "    data = json.loads(line)\n",
        "    label = data['gold_label']\n",
        "    s1 = ' '.join(extract_tokens_from_binary_parse(data['sentence1_binary_parse']))\n",
        "    s2 = ' '.join(extract_tokens_from_binary_parse(data['sentence2_binary_parse']))\n",
        "    if skip_no_majority and label == '-':\n",
        "      continue\n",
        "    yield (label, s1, s2)\n",
        "\n",
        "def get_data(fn, limit=None):\n",
        "  raw_data = list(yield_examples(fn=fn, limit=limit))\n",
        "  left = [s1 for _, s1, s2 in raw_data]\n",
        "  right = [s2 for _, s1, s2 in raw_data]\n",
        "  print(max(len(x.split()) for x in left))\n",
        "  print(max(len(x.split()) for x in right))\n",
        "\n",
        "  LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "  Y = np.array([LABELS[l] for l, s1, s2 in raw_data])\n",
        "  Y = np_utils.to_categorical(Y, len(LABELS))\n",
        "\n",
        "  return left, right, Y\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVF5aO0Uhqwp",
        "colab_type": "code",
        "outputId": "ee0ea9d4-ced8-416b-9cf6-73651b9e9832",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "path=\"drive/My Drive/Deep Learning/\"\n",
        "testpath=path+\"test.jsonl\"\n",
        "trainpath=path+\"train.jsonl\"\n",
        "\n",
        "training = get_data(trainpath)\n",
        "# validation = get_data('snli_1.0_dev.jsonl')\n",
        "test = get_data(testpath)\n",
        "\n",
        "tokenizer = Tokenizer(lower=False, filters='')\n",
        "tokenizer.fit_on_texts(training[0] + training[1])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "82\n",
            "62\n",
            "57\n",
            "30\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtEXG9-vinjU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4p2lSi1Nidf5",
        "colab_type": "code",
        "outputId": "2b50ac25-3e4e-4a09-d70d-bbd337751e4d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "# Lowest index from the tokenizer is 1 - we need to include 0 in our vocab count\n",
        "VOCAB = len(tokenizer.word_counts) + 1\n",
        "LABELS = {'contradiction': 0, 'neutral': 1, 'entailment': 2}\n",
        "#RNN = recurrent.LSTM\n",
        "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.LSTM(*args, **kwargs))\n",
        "#RNN = recurrent.GRU\n",
        "#RNN = lambda *args, **kwargs: Bidirectional(recurrent.GRU(*args, **kwargs))\n",
        "# Summation of word embeddings\n",
        "RNN = None\n",
        "LAYERS = 1\n",
        "USE_GLOVE = True\n",
        "TRAIN_EMBED = False\n",
        "EMBED_HIDDEN_SIZE = 300\n",
        "SENT_HIDDEN_SIZE = 300\n",
        "BATCH_SIZE = 512\n",
        "PATIENCE = 4 # 8\n",
        "MAX_EPOCHS = 42\n",
        "MAX_LEN = 42\n",
        "DP = 0.2\n",
        "L2 = 4e-6\n",
        "ACTIVATION = 'relu'\n",
        "OPTIMIZER = 'rmsprop'\n",
        "print('RNN / Embed / Sent = {}, {}, {}'.format(RNN, EMBED_HIDDEN_SIZE, SENT_HIDDEN_SIZE))\n",
        "print('GloVe / Trainable Word Embeddings = {}, {}'.format(USE_GLOVE, TRAIN_EMBED))\n",
        "\n",
        "to_seq = lambda X: pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_LEN)\n",
        "prepare_data = lambda data: (to_seq(data[0]), to_seq(data[1]), data[2])\n",
        "\n",
        "training = prepare_data(training)\n",
        "# validation = prepare_data(validation)\n",
        "test = prepare_data(test)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RNN / Embed / Sent = None, 300, 300\n",
            "GloVe / Trainable Word Embeddings = True, False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "afKJTFoojBdM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Build model...')\n",
        "print('Vocab size =', VOCAB)\n",
        "\n",
        "GLOVE_STORE = 'precomputed_glove.weights'\n",
        "if USE_GLOVE:\n",
        "  if not os.path.exists(GLOVE_STORE + '.npy'):\n",
        "    print('Computing GloVe')\n",
        "  \n",
        "    embeddings_index = {}\n",
        "    f = open('glove.840B.300d.txt')\n",
        "    for line in f:\n",
        "      values = line.split(' ')\n",
        "      word = values[0]\n",
        "      coefs = np.asarray(values[1:], dtype='float32')\n",
        "      embeddings_index[word] = coefs\n",
        "    f.close()\n",
        "    \n",
        "    # prepare embedding matrix\n",
        "    embedding_matrix = np.zeros((VOCAB, EMBED_HIDDEN_SIZE))\n",
        "    for word, i in tokenizer.word_index.items():\n",
        "      embedding_vector = embeddings_index.get(word)\n",
        "      if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "      else:\n",
        "        print('Missing from GloVe: {}'.format(word))\n",
        "  \n",
        "    np.save(GLOVE_STORE, embedding_matrix)\n",
        "\n",
        "  print('Loading GloVe')\n",
        "  embedding_matrix = np.load(GLOVE_STORE + '.npy')\n",
        "\n",
        "  print('Total number of null word embeddings:')\n",
        "  print(np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
        "\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, weights=[embedding_matrix], input_length=MAX_LEN, trainable=TRAIN_EMBED)\n",
        "else:\n",
        "  embed = Embedding(VOCAB, EMBED_HIDDEN_SIZE, input_length=MAX_LEN)\n",
        "\n",
        "rnn_kwargs = dict(output_dim=SENT_HIDDEN_SIZE, dropout_W=DP, dropout_U=DP)\n",
        "SumEmbeddings = keras.layers.core.Lambda(lambda x: K.sum(x, axis=1), output_shape=(SENT_HIDDEN_SIZE, ))\n",
        "\n",
        "translate = TimeDistributed(Dense(SENT_HIDDEN_SIZE, activation=ACTIVATION))\n",
        "\n",
        "premise = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "hypothesis = Input(shape=(MAX_LEN,), dtype='int32')\n",
        "\n",
        "prem = embed(premise)\n",
        "hypo = embed(hypothesis)\n",
        "\n",
        "prem = translate(prem)\n",
        "hypo = translate(hypo)\n",
        "\n",
        "if RNN and LAYERS > 1:\n",
        "  for l in range(LAYERS - 1):\n",
        "    rnn = RNN(return_sequences=True, **rnn_kwargs)\n",
        "    prem = BatchNormalization()(rnn(prem))\n",
        "    hypo = BatchNormalization()(rnn(hypo))\n",
        "rnn = SumEmbeddings if not RNN else RNN(return_sequences=False, **rnn_kwargs)\n",
        "prem = rnn(prem)\n",
        "hypo = rnn(hypo)\n",
        "prem = BatchNormalization()(prem)\n",
        "hypo = BatchNormalization()(hypo)\n",
        "\n",
        "joint = merge([prem, hypo], mode='concat')\n",
        "joint = Dropout(DP)(joint)\n",
        "for i in range(3):\n",
        "  joint = Dense(2 * SENT_HIDDEN_SIZE, activation=ACTIVATION, W_regularizer=l2(L2) if L2 else None)(joint)\n",
        "  joint = Dropout(DP)(joint)\n",
        "  joint = BatchNormalization()(joint)\n",
        "\n",
        "pred = Dense(len(LABELS), activation='softmax')(joint)\n",
        "\n",
        "model = Model(input=[premise, hypothesis], output=pred)\n",
        "model.compile(optimizer=OPTIMIZER, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S_YrJyDLjH7J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('Training')\n",
        "_, tmpfn = tempfile.mkstemp()\n",
        "# Save the best model during validation and bail out of training early if we're not improving\n",
        "callbacks = [EarlyStopping(patience=PATIENCE), ModelCheckpoint(tmpfn, save_best_only=True, save_weights_only=True)]\n",
        "model.fit([training[0], training[1]], training[2], batch_size=BATCH_SIZE, nb_epoch=MAX_EPOCHS,  callbacks=callbacks)\n",
        "\n",
        "# Restore the best found model during validation\n",
        "model.load_weights(tmpfn)\n",
        "\n",
        "loss, acc = model.evaluate([test[0], test[1]], test[2], batch_size=BATCH_SIZE)\n",
        "print('Test loss / test accuracy = {:.4f} / {:.4f}'.format(loss, acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}